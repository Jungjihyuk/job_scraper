import requests
from urllib.parse import urlparse
from job_scraper.config.logging_config import setup_logger

logger= setup_logger(__name__)

def is_https_url(url):
    """https í”„ë¡œí† ì½œ ìš”ì²­ì´ ê°€ëŠ¥í•œ ì£¼ì†Œì¸ê°€ íŒë‹¨í•˜ëŠ” í•¨ìˆ˜"""
    try:
        parsed = urlparse(url)
        return parsed.scheme == "https" and bool(parsed.netloc)
    except Exception:
        return False


def can_parse_url(url):
    """íŒŒì‹±ì´ ê°€ëŠ¥í•œ urlì¸ê°€ íŒë‹¨í•˜ëŠ” í•¨ìˆ˜ ğŸ› ï¸ê°œì„  í•„ìš”"""
    try:
        response = requests.head(url, timeout=5, allow_redirects=True)
        if 200 <= response.status_code < 400:
            return True
        else:
            return False
    except (requests.exceptions.RequestException, requests.exceptions.Timeout) as e:
        print(f"ìš”ì²­ ì‹¤íŒ¨: {e}")
        return False


def is_valid_protocol(url):
    """URLì´ ìœ íš¨í•œ í”„ë¡œí† ì½œì„ ê°€ì§€ê³  ìˆëŠ”ì§€ í™•ì¸"""
    try:
        parsed = urlparse(url)
        return parsed.scheme in ["http", "https"]
    except Exception:
        return False


def normalize_url(url):
    """URLì„ í‘œì¤€ í˜•ì‹ìœ¼ë¡œ ë³€í™˜"""
    parsed = urlparse(url)
    if not parsed.scheme:
        url = "https://" + url
    elif parsed.scheme == "http":
        url = url.replace("http://", "https://", 1)
    return url


def is_valid_url(url):
    """ìœ íš¨í•œ URLì¸ì§€ íŒë‹¨í•˜ëŠ” í•¨ìˆ˜"""
    if not is_valid_protocol(url):
        logger.info("Invalid URL protocol")
        return False
    normalized_url = normalize_url(url)
    if is_https_url(normalized_url):
        logger.info(f"Valid URL: {url}")
        return can_parse_url(normalized_url)
    else:
        logger.error(f"Invalid URL: {url}")
        return False
